% Discussion Section - Complete Implementation

\section{Discussion}

This section discusses the implications of the experimental findings, addresses limitations, and outlines directions for future research.

\subsection{Implications for Autonomous Network Defence}

The experimental results validate the viability of RL agents as components in autonomous network defence systems, albeit with important caveats about their current limitations.

\subsubsection{RL as Tunable Risk Management}

A key finding is that the asymmetric reward structure successfully influenced the DQN agent to achieve 97\% recall, prioritising attack detection over raw accuracy. This demonstrates that RL provides a mechanism for \textit{encoding organisational risk preferences} directly into the detection policy. Different security contexts require different trade-offs:

\begin{itemize}
    \item \textbf{High-Security Environments}: Critical infrastructure may accept higher false positive rates in exchange for maximising threat detection. The reward structure could be tuned with FN penalty of $-50$ to push recall even higher.
    \item \textbf{User-Experience Sensitive Contexts}: Consumer-facing services may require lower false positive rates. Adjusting the FP penalty from $-1$ to $-5$ would shift the agent's behaviour accordingly.
\end{itemize}

This tunability is not easily achievable with standard supervised learning, where the loss function is typically fixed (e.g., cross-entropy) and class weights are the only adjustment mechanism.

\subsubsection{Reducing Dwell Time Through Automation}

The trained RL agent can evaluate network flows in milliseconds, significantly faster than human analyst review. In Security Operations Centres (SOCs) facing alert fatigue, an RL-based triage system could function as a ``Level 1 analyst'' handling initial classification, escalating only high-confidence threats to human review. This has potential to reduce dwell time—the window between initial compromise and detection—from hours or days to seconds.

\subsection{Comparison with Related Work}

The DQN implementation achieved comparable results to similar studies in the literature. Alavizadeh et al. \cite{alavizadeh2022deepq} reported that DQN-based IDS could provide ``ongoing auto-learning capability,'' which aligns with our findings regarding the agent's ability to adjust its policy based on reward feedback.

The PPO agent's underwhelming performance (48\% recall) differs from Liang et al. \cite{liang2024ppo}, who achieved 97\% accuracy with PPO. This discrepancy likely stems from differences in training duration; our 200,000 timesteps may be insufficient compared to the extended training regimes in comparable studies.

\subsection{Limitations}

Several limitations must be acknowledged:

\begin{enumerate}
    \item \textbf{Static Dataset Training}: The RL agents were trained on the CIC-IDS2017 dataset in an offline manner. While framed as ``reinforcement learning,'' the training process more closely resembles offline RL or contextual bandits, as the agent receives immediate feedback from ground-truth labels rather than delayed, partial, or noisy signals typical of production environments.
    
    \item \textbf{Simplified Action Space}: The binary ``Allow/Block'' action space, while appropriate for proof-of-concept, does not capture the full range of responses available in real IDS deployments (rate limiting, quarantine, escalation, honeypot redirection).
    
    \item \textbf{Benchmark Dataset Artifacts}: CIC-IDS2017, while modern compared to NSL-KDD, still represents a ``snapshot'' of network conditions from 2017. Real-world implementations would face concept drift as attack patterns evolve.
    
    \item \textbf{Computational Overhead}: Training RL agents required significantly more computational resources than fitting Random Forest or XGBoost models. For resource-constrained environments, ML baselines may remain preferable.
    
    \item \textbf{Inference Latency}: While not formally benchmarked in this study, deep neural network inference may introduce latency compared to decision tree-based classifiers. This is acknowledged as an area for future investigation.
\end{enumerate}

\subsection{Justification of the RL Framework}

A valid question is whether the RL framing is necessary, or whether cost-sensitive classification could achieve similar results. We argue that RL provides distinct advantages:

\begin{enumerate}
    \item \textbf{Reward Flexibility}: The reward function can encode complex, multi-objective preferences beyond what loss function weighting allows. Different attack types could receive different reward magnitudes based on threat severity.
    
    \item \textbf{Foundation for Online Learning}: While this study used offline training, the RL framework provides a natural extension path to online learning, where agents could update from incident confirmations, analyst feedback, or red-team exercises.
    
    \item \textbf{Sequential Decision Context}: Although each flow classification is technically independent, production IDS must make sequences of decisions under uncertainty. The RL episodic training structure better reflects this operational reality than i.i.d. supervised learning.
\end{enumerate}

\subsection{Future Research Directions}

Based on the findings and limitations identified, several directions for future work are proposed:

\begin{itemize}
    \item \textbf{Extended PPO Training}: Increase PPO training to 500,000--1,000,000 timesteps to determine whether additional training resolves the performance gap with DQN.
    
    \item \textbf{Curriculum Learning}: Train agents progressively on easier cases (benign traffic) before introducing attack patterns. This may improve sample efficiency and final performance, particularly for PPO.
    
    \item \textbf{Multi-Agent Systems}: Investigate specialised agents for different attack categories (DDoS detector, Web Attack detector) that collaborate through an ensemble or hierarchical structure.
    
    \item \textbf{Continual Learning}: Address the catastrophic forgetting problem through techniques like Elastic Weight Consolidation (EWC) \cite{kirkpatrick2017overcoming}, enabling agents to retain knowledge of old threats while adapting to new ones.
    
    \item \textbf{CybORG++ Integration}: Migrate the environment to the CybORG++ framework to enable proactive defence scenarios beyond simple detection, including automated response and lateral movement mitigation.
    
    \item \textbf{Adversarial Evaluation}: Test the trained agents against adversarial evasion attempts, where attackers craft traffic specifically designed to evade detection.
\end{itemize}
