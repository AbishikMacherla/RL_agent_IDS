Intrusion detection has long been a challenge for cybersecurity professionals of the operations industry \cite{buczak2016survey}. As cyber threats continue to evolve in both scale and sophistication, the need for robust and adaptive Intrusion Detection Systems (IDS) has never been greater. The persistence of high-impact breaches is fundamentally tied to ``dwell time'' the window between initial system compromise and final containment. Recent experimental evidence indicates that the global average duration to identify and contain a data breach in 2025 has settled at approximately 241 days \cite{ibm2025breach}. These figures underscore a critical failure in traditional, static security architectures and mandate a transition toward autonomous network defence systems capable of reasoning and responding at machine speed.

Traditional IDS approaches, including signature-based and anomaly-based detection methods, have shown significant limitations in identifying novel attacks and adapting to rapidly changing threat landscapes. Signature-based systems maintain databases of known attack fingerprints, achieving high accuracy for documented threats but remaining fundamentally blind to zero-day exploits \cite{alam2025zeroday}. Anomaly-based systems model normal behaviour and flag deviations, theoretically enabling novel threat detection, but suffer from high false positive rates leading to ``alert fatigue'' in Security Operations Centers (SOCs) \cite{shanmugam2025imbalance}. Recent years have witnessed a surge in the application of machine learning to cybersecurity, with supervised models achieving near-perfect accuracy on benchmark datasets \cite{mondragon2025advanced}. However, these solutions are not without limitations: supervised machine learning models, while achieving high accuracy on training distributions, remain fundamentally static after training, unable to adapt to evolving attacker tactics without complete retraining.

This has led researchers to explore more advanced AI techniques, with Reinforcement Learning (RL) emerging as the most promising paradigm for achieving autonomous defence \cite{nguyen2021deep}. Unlike traditional machine learning approaches that learn from static datasets of labelled examples, an RL agent learns through trial-and-error interaction with an environment, receiving feedback in the form of rewards for its actions \cite{sutton2020reinforcement}. This feedback-driven learning mechanism offers a compelling property for intrusion detection: the ability to continuously refine detection policies based on the success or failure of interventions. When an RL agent misclassifies a threat, it receives a negative reward and adjusts its behaviour accordingly, potentially enabling adaptation to new attack patterns without complete model retraining.

The motivation behind this study stems from the need to address the limitations of current IDS technologies and leverage the adaptive properties of reinforcement learning in the cybersecurity domain. In conventional SOC environments, human-driven detection and verification processes can result in dwell times measured in hours or even days, providing attackers enough opportunity to establish persistence or exfiltrate sensitive data. Organisations utilising extensive AI and automation in their security operations have demonstrated an ability to shorten the breach lifecycle by up to 80 days, significantly mitigating long-term costs \cite{ibm2025breach}. An autonomous RL agent, by contrast, can evaluate and respond to potential threats in seconds, potentially reducing this window of vulnerability significantly.
\newpage
On this investigation, the following research objectives were:
\begin{enumerate}
    \item To implement and compare two Deep Reinforcement Learning algorithms: Deep Q-Network (DQN) and Proximal Policy Optimisation (PPO) for network intrusion detection within a simulated environment.
    \item To evaluate the RL agents' performance against traditional supervised machine learning baselines (Random Forest and XGBoost) using the CIC-IDS2017 benchmark dataset.
    \item To assess the agents' generalisation capabilities through zero-day attack simulation, where specific attack categories are withheld during training and evaluated during testing.
    \item To analyse the agents' response to asymmetric reward structures, examining whether reward engineering can effectively prioritise high recall (threat detection) over raw classification accuracy.
\end{enumerate}

This work contributes to the growing field of Autonomous Cyber Defence (ACD). By implementing an OpenAI Gym-compatible environment inspired by the \textit{CybORG++} framework design principles \cite{emerson2024cyborgplusplus}, we address limitations of unstable legacy environments found in previous research. The project demonstrates the viability of RL agents not merely as classifiers, but as adaptive decision-makers whose risk tolerance can be tuned through reward engineering alone. A Streamlit-based dashboard provides real-time training visualisation and hyperparameter tuning capabilities, enabling iterative refinement of agent behaviour.

The structure of this paper is as follows. Section 2 provides background information on traditional intrusion detection approaches, the application of machine learning in network security, and the emerging role of reinforcement learning in autonomous cyber defence. Section 3 outlines the experimental methodology, which includes environment setup, dataset preprocessing, and the implementation of DQN, PPO, and machine learning baseline models. Section 4 presents the experimental results, with a focus on performance comparisons across standard and zero-day scenarios. Section 5 discusses the implications of the findings, addresses limitations and challenges, and outlines directions for future research. Section 6 concludes by summarising the key findings and their significance for adaptive intrusion detection.
