\section{Intrusion Detection Systems (IDS)}

An Intrusion Detection System (IDS) acts like a ``digital alarm system'' for a computer network. It's main job is to monitor all the activity on the network traffic and identify any potential security breaches or malicious activities. When it detects something suspicious, it logs the event and alerts a human security analyst. As the volume and complexity of cyber-attacks grow, these automated systems are essential for protecting critical data \cite{buczak2016survey}.
\newline
There are two main types of IDS, and some modern systems use a hybrid approach \cite{wang2024research}.\
\newline
\textbf{Signature-Based IDS} acts similarly to antivirus software, maintaining a database of ``signatures'' or digital footprints belonging to known cyber-attacks. It scans network traffic against this list, making it extremely accurate for catching known threats. However, its significant weakness is inflexibility; it cannot detect ``zero-day'' attacks or novel threats that lack a pre-existing signature.
\newline
\textbf{Anomaly-Based IDS} works by building a model of normal network traffic behaviour rather than looking for specific attack signatures. It flags activity that deviates from this baseline. In theory, this approach allows for the detection of zero-day attacks. However, it often suffers from unreliability due to high rates of false positives. Harmless but unusual actions, such as an employee logging in at an unusual time, may be flagged, leading to ``alert fatigue'' for human analysts who may ignore real threats amidst the noise. \newline
\linebreak
This project focuses on improving the anomaly-based approach, aiming to retain its ability to detect new threats while mitigating the issues of high false positives and inflexibility.

\section{Reinforcement Learning for Adaptive Defence}

Researchers and organisations have turned to a new kind of artificial intelligence to address these limitations. This section introduces Reinforcement Learning (RL), which is the core technology for this project.

\subsection{Machine Learning vs Reinforcement Learning}

Standard Supervised Machine Learning (ML) trains a model using large, labelled datasets, such as the CIC-IDS2017 dataset used in this work \cite{maseer2021benchmarking}. These models learn patterns by processing examples of ``normal'' and ``attack'' traffic, resulting in a static artifact that classifies data based on its training. A significant limitation of this approach is its inability to adapt post-training.
\newline
Reinforcement Learning (RL), in contrast, learns from experience rather than a static ``answer key'' \cite{sutton2020reinforcement}. An RL system learns through trial-and-error, interacting with an environment to maximise a reward signal.
\clearpage
\subsection{The RL Agent and Agentic Security}

The distinction between a static ML ``model'' and an RL ``agent'' is central to this research. While an ML model is a static file that makes predictions, an RL agent is a dynamic system capable of action and learning. The agent may use a deep neural network as its ``brain,'' but acts as an autonomous entity that observes and reacts to its surroundings.
\newline
In cybersecurity, ``Agentic RL'' systems function as autonomous defenders. Unlike static rule-based systems, an agentic system enables continuous monitoring and adaptive responses. The agent observes the \textbf{state} of the network traffic, selects an \textbf{action} (such as flagging, blocking, or passing traffic), and receives a \textbf{reward} based on the outcome (e.g., positive reinforcement for stopping an attack, negative for a false positive). A persistent challenge for these agents is ``partial observability,'' where the agent must infer threats from limited data, as it may not see the entire network state or hidden attacker movements.

\subsection{Algorithm Selection: DQN vs. PPO}

Choosing the right RL algorithm is critical for the stability and performance of an IDS. Two dominant algorithms in Deep Reinforcement Learning (DRL) are frequently compared:
\newline
\textbf{Deep Q-Network (DQN)} is a value-based method that learns the value of taking a specific action in a specific state \cite{hsu2020deep}. It is particularly well-suited for discrete action spaces, such as an IDS deciding to simply ``Block'' or ``Allow'' a packet. Alavizadeh et al. \cite{alavizadeh2022deepq} demonstrated that DQN-based IDS can provide ``ongoing auto-learning capability'' that detects different types of network intrusions through trial-and-error. While sample-efficient, DQN can sometimes be unstable during training.
\newline
\textbf{Proximal Policy Optimization (PPO)}, introduced by Schulman et al. \cite{schulman2017proximal}, is a policy-gradient method. It is often praised for its stability and ease of tuning compared to other policy gradient methods. Liang et al. \cite{liang2024ppo} recently proposed a PPO-based model that allows for dynamic adaptation of defensive strategies in response to evolving environmental patterns, demonstrating that the agent can ``learn and optimise detection strategies'' through continuous interaction. For simple discrete tasks, PPO may be computationally heavier than DQN, but its ``trust region'' update mechanism prevents destabilising policy changes \cite{bates2023reward}.
\newline
For this project, both algorithms are implemented to compare their performance characteristics: DQN for its sample efficiency in discrete classification, and PPO for its training stability and adaptability.

\section{Enhancing IDS with RL}

The primary advantage of combining an RL agent with an IDS is adaptability. When a brand-new attack appears, traditional Signature-Based IDSs fail due to the lack of a pre-existing signature, and Static ML Baselines often fail because they were not trained on the specific pattern.
\newline
An RL agent might also fail initially. However, through feedback (simulated or human-in-the-loop), it receives a negative reward for the failure. The agent updates its policy and learns from this mistake, making it more likely to detect similar patterns in the future. This process also reduces false positives; if the agent flags harmless user activity, a negative reward teaches it to adjust its definition of ``normal'' behaviour for that specific network.

\subsection{Avoiding Dwell Time}

In cyberdefence, ``Dwell time'', the time an attack remains undetected in a network is a critical metric. Human detection often yields dwell times measured in hours or days. In contrast, an automated RL agent can flag anomalies in seconds. By automating the initial detection and triage, RL agents can drastically reduce this window of vulnerability, preventing attackers from establishing persistence or exfiltrating data.

\subsection{Improving the RL Model Accuracy}

To make RL-based IDS practical, accuracy must be high. Research focuses on several areas to improve this. 
\newline
\textbf{Feature Engineering} involves selecting the optimal features from network traffic; techniques like ID-RDRL \cite{ren2022id} combine recursive feature elimination with Deep RL to improve the detection of unknown attacks. 
\newline
\textbf{Reward Tuning} is also essential, as simple positive/negative rewards can be too sparse to guide learning effectively. Tuning involves providing intermediate rewards, such as penalising false negatives (missed attacks) more heavily than false positives. 
\newline
\textbf{Handling Class Imbalance} is crucial, as attack traffic is rare compared to normal traffic. Shanmugam et al. \cite{shanmugam2025imbalance} provide a comprehensive evaluation of class imbalance techniques in IDS, demonstrating that resampling strategies significantly affect model reliability. Techniques such as oversampling, undersampling, or using synthetic attacks in the training environment help the agent learn to identify rare threats.

\subsection{Zero-Day Attack Detection}

A primary strength for RL-based IDS is the identification of zero-day vulnerabilities security weaknesses that have not yet been disclosed or patched. Traditional methods, which rely on historical data, are fundamentally limited in detecting novel threats. Alam et al. \cite{alam2025zeroday} propose a Deep Reinforcement Learning-based NIDS designed specifically for zero-day attack detection, ``utilising learned features from other known attacks'' to identify previously unseen malicious patterns. Their approach treats zero-day attacks as an anomaly detection problem, excluding specific attack categories during training and evaluating the agent's ability to detect these held-out attacks during testing. This methodology directly informs our experimental design.

\subsection{The Datasets}

A major criticism of many IDS studies is the use of outdated dataset like NSL-KDD. These datasets lack the traffic diversity and modern attack patterns found in today's networks \cite{ring2019survey}.
\newline
This project utilizes the CIC-IDS2017 dataset \cite{sharafaldin2018toward}. Unlike its predecessors, CIC-IDS2017 includes valid pcap data generated from a realistic testbed, containing benign traffic alongside modern attacks such as DDoS, Brute Force, and Web Application attacks. This ensures that the RL agent is trained on patterns relevant to current cybersecurity threats rather than historical artifacts.
\clearpage

\subsection{Challenges in Real life}

Deploying an RL agent into a live network introduces unique risks that supervised models do not face.
\newline
\textbf{Catastrophic Forgetting}: As an RL agent continues to learn from new attacks, it risks overwriting the knowledge it gained about older attacks as known as catastrophic forgetting. Techniques like Elastic Weight Consolidation (EWC) \cite{kirkpatrick2017overcoming} or continual learning strategies \cite{khetarpal2020survey} are required to ensure the agent retains its past experience.
\newline
\textbf{The ``Cold Start'' Problem}: An untrained RL agent explores by trial-and-error, which is dangerous in a live network (e.g., randomly blocking legitimate users). To mitigate this, agents must undergo a ``pre-training'' phase in a simulated environment (offline RL) before being deployed to the real world \cite{pan2025survey}.

\section{Related Research}

In todayâ€™s AI and Cyber era, RL for cybersecurity is growing. Yang et al. \cite{yang2024survey} provide a comprehensive survey of Deep RL methods for intrusion detection, highlighting architectures and challenges.
\newline
\textbf{Anomaly Detection Approaches}: Hsu and Matsuoka \cite{hsu2020deep} demonstrate implementations of Deep Q Networks (DQN) for anomaly detection, proving the concept. Malik and Singh Saini \cite{malik2023network} showed that RL agents could outperform traditional ML models. Suwannalai and Polprasert \cite{suwannalai2020network} also explored adversarial RL with Deep Q-Networks.
\newline
\textbf{Multi-Agent and Generalisable Approaches}: Tellache et al. \cite{tellache2024multi} have expanded this to Multi-agent systems. Latest research by Dudman and Bull \cite{dudman2025towards} on Generalisable Cyber Defence Agents shows the push towards agents that can operate in complex, real-world environments.
\newline
\textbf{Real-World Applications}: Beyond academic prototypes, major tech organisations are investing in RL for security. Use cases include Microsoft's \textit{CyberBattleSim}, which uses RL agents to simulate lateral movement in a network, helping defenders understand attacker behaviour. Foley et al. \cite{foley2022autonomous} explored the practical implementation of PPO for real-time network remediation, focusing on the agent's ability to balance defensive actions with network availability. This moves RL from purely detection (IDS) to proactive defence and autonomous cyber operations (ACO).
\newline
\textbf{Comparative Studies}: Recent work by Mondragon Guadarrama et al. \cite{mondragon2025advanced} presents a benchmark of fourteen preprocessed datasets to address the lack of standardisation in IDS research, evaluating multiple algorithm categories including Random Forest and XGBoost. This comprehensive approach validates the importance of comparing RL agents against strong ML baselines. Fathi et al. \cite{fathi2025rlids} provide a taxonomy of modern RL techniques for intrusion detection, highlighting the transition from single-agent to multi-agent and adversarial frameworks.

\section{Problem Formulation and Threat Model}

This section formally defines the intrusion detection problem as a decision-making task amenable to reinforcement learning and specifies the threat model under which the autonomous defence system operates.

\subsection{Threat Model}

A rigorous threat model is essential to justify claims of ``autonomous defence.'' The following assumptions define the security context:

\subsubsection{Attacker Capabilities}

The adversary is assumed to possess the following capabilities, informed by contemporary threat intelligence \cite{syairozi2025challenges, mambwe2024limitations}:

\begin{itemize}
    \item \textbf{Novel Attack Generation}: Attackers can craft previously unseen attack patterns (zero-day exploits) that lack signatures in traditional databases.
    
    \item \textbf{Evasion Techniques}: Sophisticated attackers employ polymorphic malware, steganographic techniques, and encrypted payloads to evade pattern-based detection \cite{mambwe2024limitations}.
    
    \item \textbf{Adversarial Manipulation}: In advanced scenarios, attackers may attempt to poison the learning process or craft adversarial inputs specifically designed to evade ML-based detection \cite{merzouk2025adversarial}.
    
    \item \textbf{Volume Attacks}: Distributed Denial of Service (DDoS) attacks can generate massive traffic volumes to overwhelm both network infrastructure and detection systems.
\end{itemize}

\subsubsection{Defender Constraints}

The autonomous defence system operates under the following constraints:

\begin{itemize}
    \item \textbf{Limited Visibility}: The agent observes only flow-level features extracted from network traffic, not full packet payloads or system-level indicators. This represents partial observability of the true network state.
    
    \item \textbf{Analyst Bandwidth}: Human security analysts have limited capacity to review alerts. The system must minimise false positives while maximising true threat detection.
    
    \item \textbf{Acceptable Disruption Rate}: Blocking legitimate traffic has business costs. The defender must maintain an acceptable false positive rate to avoid disrupting normal operations.
    
    \item \textbf{Latency Requirements}: Detection and response must occur within milliseconds to prevent attackers from establishing persistence or exfiltrating data \cite{ibm2025breach}.
\end{itemize}

\subsection{Formal RL Formulation}

The intrusion detection task is formalised as a Markov Decision Process (MDP), with acknowledgment of partial observability characteristics:

\subsubsection{State/Observation Space}

The observation $o_t \in \mathcal{O}$ at timestep $t$ consists of an 80-dimensional feature vector extracted from a network flow:
\begin{equation}
o_t = [f_1, f_2, \ldots, f_{80}]
\end{equation}
where features include flow duration, packet counts, byte statistics, inter-arrival times, and flag counts. All features are normalised to $[0, 1]$ through min-max scaling.

\subsubsection{Action Space}

The action space $\mathcal{A}$ is discrete with two elements:
\begin{equation}
\mathcal{A} = \{0: \text{Allow}, 1: \text{Block}\}
\end{equation}
This binary formulation represents the fundamental classification decision. Extended action spaces (rate-limit, quarantine, escalate) are considered as future work.

\subsubsection{Reward Function}

The reward function $R(s, a, s')$ encodes the security-first priority through asymmetric penalties:

\begin{equation}
R(a, y) = \begin{cases}
+10 & \text{if } a = 1 \text{ and } y = 1 \text{ (True Positive)} \\
+1 & \text{if } a = 0 \text{ and } y = 0 \text{ (True Negative)} \\
-1 & \text{if } a = 1 \text{ and } y = 0 \text{ (False Positive)} \\
-10 & \text{if } a = 0 \text{ and } y = 1 \text{ (False Negative)}
\end{cases}
\end{equation}

where $y \in \{0, 1\}$ is the ground truth label. This 10:1 asymmetry between missed attacks (FN) and false alarms (FP) reflects the security principle that undetected intrusions pose greater risk than minor user disruption.

\subsubsection{Justification of RL Framework}

A legitimate question is whether reinforcement learning is necessary for this task, or whether cost-sensitive classification would suffice. Several factors justify the RL approach:

\begin{enumerate}
    \item \textbf{Reward Engineering Flexibility}: Unlike loss function weighting in supervised learning, RL rewards can encode complex, multi-objective preferences. Different attack types could receive different reward magnitudes based on threat severity, and the reward structure can be modified without retraining the underlying model architecture.
    
    \item \textbf{Foundation for Online Adaptation}: While this study employs offline training on static datasets, the RL framework provides a natural pathway to online learning. Future deployments could update from analyst feedback, incident confirmations, or red-team exercises without architectural changes \cite{jamshidi2025drl}.
    
    \item \textbf{Demonstrated Efficacy}: Recent systematic reviews confirm that DRL ``greatly improves IDS performance in IoT: it boosts detection accuracy, reduces false alarms, and adapts in real time'' \cite{jamshidi2025drl}. Military and defense organisations have achieved successful proof-of-concepts where RL agents outperform human-defined rule sets \cite{milesfarmer2024arcd}.
    
    \item \textbf{Simulation Environment Compatibility}: The RL formulation enables training in cyber simulation environments like CybORG++ \cite{emerson2024cyborgplusplus} and CyberBattleSim \cite{kunz2022cyberbattlesim}, facilitating curriculum learning and multi-agent training scenarios \cite{claypoole2025curriculum}.
\end{enumerate}

\subsubsection{Acknowledgment of Limitations}

The current implementation operates on a static, labelled dataset with immediate feedback, which more closely resembles offline RL or contextual bandits than fully online sequential decision-making. The ``ground truth'' labels provide an idealised reward signal that would not be available in production deployments, where feedback is delayed, partial, and noisy (e.g., analyst confirmations may arrive hours after initial detection). This limitation is explicitly acknowledged, and the experimental results should be interpreted as a proof-of-concept for the reward engineering approach rather than a claim of production-ready autonomous defence.

