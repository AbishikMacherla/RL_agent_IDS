The aim of this project is to develop and evaluate a proof-of-concept Intrusion Detection System (IDS) enhanced by Reinforcement Learning (RL) agents for autonomous network defence. Traditional IDSs rely on static signatures or supervised machine learning models that lack the ability to adapt to novel, zero-day threats in real-time. This research integrates RL agents into a simulated network defence environment to create adaptive response mechanisms capable of continuous learning from feedback. Leveraging the CIC-IDS2017 dataset and OpenAI Gym-compatible environment design, we implement and compare two RL algorithms of Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) against traditional machine learning baseline including XGBoost. The experimental methodology includes standard classification scenarios and zero-day attack simulations where specific attack types are excluded during training to evaluate generalization capabilities. While the machine learning baselines achieved near-perfect accuracy, the RL agents demonstrated a security-first approach with significantly higher recall rates, effectively prioritizing threat detection at the cost of higher false positives. These results confirm that RL agents can be tuned towards high-security postures through asymmetric reward engineering, offering a promising direction for autonomous network defence where missing an attack carries greater consequence than a false alarm.
