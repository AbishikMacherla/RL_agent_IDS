{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb7db24",
   "metadata": {},
   "source": [
    "# RL-Enhanced IDS — Results Visualisation\n",
    "**Project**: RL-Enhanced Intrusion Detection System for Autonomous Network Defence  \n",
    "**Author**: Abishik Macherla Vijayakrishna | 40594078\n",
    "\n",
    "This notebook generates all dissertation figures from experiment results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d63732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.15, rc={\n",
    "    \"figure.dpi\": 150,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "})\n",
    "\n",
    "PROJECT_DIR = \"/home/abishik/HONOURS_PROJECT\"\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, \"results\")\n",
    "FIGURES_DIR = os.path.join(RESULTS_DIR, \"figures\")\n",
    "os.makedirs(FIGURES_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4981bcaf",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e22960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scenario results\n",
    "with open(os.path.join(RESULTS_DIR, \"all_scenarios_results.json\")) as f:\n",
    "    scenarios = json.load(f)\n",
    "print(f\"Loaded scenarios: {list(scenarios.keys())}\")\n",
    "\n",
    "# Load DQN experiment results\n",
    "with open(os.path.join(RESULTS_DIR, \"dqn_experiments.json\")) as f:\n",
    "    dqn_experiments = json.load(f)\n",
    "print(f\"Loaded {len(dqn_experiments)} DQN experiments\")\n",
    "\n",
    "# Colour palette\n",
    "MODEL_COLORS = {\n",
    "    \"Random Forest\": \"#27ae60\", \"XGBoost\": \"#2980b9\",\n",
    "    \"DQN\": \"#e67e22\", \"DQN (Standard)\": \"#e67e22\",\n",
    "    \"DQN (No DDoS)\": \"#f39c12\", \"DQN (No Web)\": \"#f39c12\",\n",
    "}\n",
    "\n",
    "SCENARIO_TITLES = {\n",
    "    \"scenario_1\": \"Scenario 1: Standard Classification (CIC-IDS2017)\",\n",
    "    \"scenario_2\": \"Scenario 2: Zero-Day DDoS Detection\",\n",
    "    \"scenario_3\": \"Scenario 3: Zero-Day Web Attack Detection\",\n",
    "    \"scenario_4\": \"Scenario 4: Cross-Dataset Generalisation (2017→2023)\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5bbbe",
   "metadata": {},
   "source": [
    "## 2. Scenario Comparison Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d0b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "metric_colors = [\"#1abc9c\", \"#3498db\", \"#e74c3c\", \"#9b59b6\"]\n",
    "\n",
    "for s_key, s_data in scenarios.items():\n",
    "    if s_key not in SCENARIO_TITLES:\n",
    "        continue\n",
    "    models = list(s_data.keys())\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(max(8, len(models) * 2.2), 5))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        vals = [s_data[m].get(metric, 0) for m in models]\n",
    "        bars = ax.bar(x + i * width, vals, width, label=metric,\n",
    "                      color=metric_colors[i], edgecolor=\"white\")\n",
    "        for bar, val in zip(bars, vals):\n",
    "            ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.8,\n",
    "                    f\"{val:.1f}\", ha=\"center\", va=\"bottom\", fontsize=8, fontweight=\"bold\")\n",
    "\n",
    "    ax.set_xticks(x + 1.5 * width)\n",
    "    ax.set_xticklabels(models, fontsize=10)\n",
    "    ax.set_ylabel(\"Score (%)\")\n",
    "    ax.set_ylim(0, 108)\n",
    "    ax.set_title(SCENARIO_TITLES[s_key], fontsize=14, pad=15)\n",
    "    ax.legend(loc=\"upper right\", fontsize=9)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(FIGURES_DIR, f\"fig_{s_key}_comparison.png\"), bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1eae57",
   "metadata": {},
   "source": [
    "## 3. Cross-Scenario F1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7a8295",
   "metadata": {},
   "outputs": [],
   "source": [
    "names, best_f1, best_models, dqn_f1 = [], [], [], []\n",
    "for s_key in [\"scenario_1\", \"scenario_2\", \"scenario_3\", \"scenario_4\"]:\n",
    "    if s_key not in scenarios: continue\n",
    "    data = scenarios[s_key]\n",
    "    names.append(s_key.replace(\"scenario_\", \"S\"))\n",
    "    best_m = max(data, key=lambda m: data[m].get(\"F1\", 0))\n",
    "    best_f1.append(data[best_m][\"F1\"])\n",
    "    best_models.append(best_m)\n",
    "    dqn_keys = [k for k in data if \"DQN\" in k]\n",
    "    best_dqn = max(dqn_keys, key=lambda k: data[k][\"F1\"]) if dqn_keys else None\n",
    "    dqn_f1.append(data[best_dqn][\"F1\"] if best_dqn else 0)\n",
    "\n",
    "x = np.arange(len(names))\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(x - 0.175, best_f1, 0.35, label=\"Best Model\", color=\"#27ae60\")\n",
    "ax.bar(x + 0.175, dqn_f1, 0.35, label=\"Best DQN\", color=\"#e67e22\")\n",
    "for i in range(len(names)):\n",
    "    ax.text(x[i] - 0.175, best_f1[i] + 1, f\"{best_f1[i]:.1f}%\n",
    "({best_models[i]})\",\n",
    "            ha=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "    ax.text(x[i] + 0.175, dqn_f1[i] + 1, f\"{dqn_f1[i]:.1f}%\",\n",
    "            ha=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names)\n",
    "ax.set_ylabel(\"F1 Score (%)\")\n",
    "ax.set_title(\"Cross-Scenario F1 Performance Summary\", fontsize=14, pad=15)\n",
    "ax.legend()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(FIGURES_DIR, \"fig_cross_scenario_summary.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbdefa",
   "metadata": {},
   "source": [
    "## 4. DQN Hyperparameter Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "names, f1s, fps, fns, precs, recs = [], [], [], [], [], []\n",
    "for exp_id, exp in dqn_experiments.items():\n",
    "    r = exp[\"results\"]\n",
    "    names.append(exp.get(\"name\", exp_id).replace(\"Exp \", \"E\"))\n",
    "    f1s.append(r[\"F1\"]); fps.append(r[\"FP\"]); fns.append(r[\"FN\"])\n",
    "    precs.append(r[\"Precision\"]); recs.append(r[\"Recall\"])\n",
    "\n",
    "# F1 progression\n",
    "colors = [\"#e74c3c\"] * 5 + [\"#2980b9\"] * 3\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(names, f1s, color=colors[:len(names)], edgecolor=\"white\")\n",
    "for bar, val in zip(bars, f1s):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.5,\n",
    "            f\"{val:.1f}%\", ha=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "ax.axvline(x=4.5, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.text(2, max(f1s)+5, \"Phase 1: Reward Tuning\", ha=\"center\", fontsize=11,\n",
    "        color=\"#e74c3c\", fontweight=\"bold\")\n",
    "ax.text(6, max(f1s)+5, \"Phase 2: Architecture\", ha=\"center\", fontsize=11,\n",
    "        color=\"#2980b9\", fontweight=\"bold\")\n",
    "ax.set_ylabel(\"F1 Score (%)\")\n",
    "ax.set_ylim(0, max(f1s)+10)\n",
    "ax.set_title(\"DQN Experiments — F1 Score Comparison\", fontsize=14, pad=15)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(FIGURES_DIR, \"fig_dqn_f1_progression.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ef476",
   "metadata": {},
   "source": [
    "## 5. Precision vs Recall Trade-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793627c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sc = ax.scatter(recs, precs, c=f1s, cmap=\"RdYlGn\", s=150, edgecolors=\"black\", linewidth=0.8)\n",
    "short_names = [n.split(\": \")[-1] if \": \" in n else n for n in names]\n",
    "for i, name in enumerate(short_names):\n",
    "    ax.annotate(name, (recs[i], precs[i]), textcoords=\"offset points\",\n",
    "                xytext=(0, 12), ha=\"center\", fontsize=8, fontweight=\"bold\")\n",
    "fig.colorbar(sc, ax=ax, label=\"F1 Score (%)\")\n",
    "ax.set_xlabel(\"Recall (%)\")\n",
    "ax.set_ylabel(\"Precision (%)\")\n",
    "ax.set_title(\"DQN — Precision vs Recall Trade-off\", fontsize=14, pad=15)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(FIGURES_DIR, \"fig_dqn_precision_recall.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01eadf",
   "metadata": {},
   "source": [
    "## 6. Error Analysis (FP vs FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d1cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(names))\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(x - 0.175, fps, 0.35, label=\"False Positives\", color=\"#e74c3c\")\n",
    "ax.bar(x + 0.175, fns, 0.35, label=\"False Negatives\", color=\"#3498db\")\n",
    "for i in range(len(names)):\n",
    "    ax.text(x[i]-0.175, fps[i]+max(fps)*0.02, f\"{fps[i]:,}\", ha=\"center\", fontsize=7)\n",
    "    ax.text(x[i]+0.175, fns[i]+max(fns)*0.02, f\"{fns[i]:,}\", ha=\"center\", fontsize=7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names, fontsize=9)\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"DQN — False Positives vs False Negatives\", fontsize=14, pad=15)\n",
    "ax.legend()\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(FIGURES_DIR, \"fig_dqn_fp_fn_analysis.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8fa99",
   "metadata": {},
   "source": [
    "## 7. Reward Structure Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32093e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1 only (first 5 experiments)\n",
    "p1_names, p1_recs, p1_precs, p1_f1s, p1_rewards = [], [], [], [], []\n",
    "for i, (exp_id, exp) in enumerate(dqn_experiments.items()):\n",
    "    if i >= 5: break\n",
    "    r, rw = exp[\"results\"], exp.get(\"rewards\", {})\n",
    "    p1_names.append(exp.get(\"name\", exp_id).split(\": \")[-1] if \": \" in exp.get(\"name\",\"\") else exp_id)\n",
    "    p1_recs.append(r[\"Recall\"]); p1_precs.append(r[\"Precision\"]); p1_f1s.append(r[\"F1\"])\n",
    "    p1_rewards.append(f\"+{rw.get(\"tp\",0)}/+{rw.get(\"tn\",0)}/{rw.get(\"fn\",0)}/{rw.get(\"fp\",0)}\")\n",
    "\n",
    "x = np.arange(len(p1_names))\n",
    "width = 0.25\n",
    "fig, ax = plt.subplots(figsize=(10, 5.5))\n",
    "ax.bar(x - width, p1_recs, width, label=\"Recall\", color=\"#e74c3c\")\n",
    "ax.bar(x, p1_precs, width, label=\"Precision\", color=\"#3498db\")\n",
    "ax.bar(x + width, p1_f1s, width, label=\"F1\", color=\"#2ecc71\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{n}\n",
    "{rl}\" for n, rl in zip(p1_names, p1_rewards)], fontsize=9)\n",
    "ax.set_ylabel(\"Score (%)\")\n",
    "ax.set_ylim(0, 105)\n",
    "ax.set_title(\"Impact of Reward Structure on DQN Detection Behaviour\", fontsize=14, pad=15)\n",
    "ax.legend(fontsize=10, loc=\"lower right\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(FIGURES_DIR, \"fig_reward_impact.png\"), bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e5878",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386e2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s_key, s_data in scenarios.items():\n",
    "    if s_key not in SCENARIO_TITLES: continue\n",
    "    models = list(s_data.keys())\n",
    "    cols = min(len(models), 3)\n",
    "    rows = (len(models) + cols - 1) // cols\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 4.5*rows))\n",
    "    if len(models) == 1: axes = np.array([axes])\n",
    "    axes = np.array(axes).flatten()\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        ax = axes[i]\n",
    "        cm = np.array(s_data[model].get(\"ConfusionMatrix\", [[0,0],[0,0]]))\n",
    "        total = max(cm.sum(), 1)\n",
    "        annot = np.array([\n",
    "            [f\"TN\n",
    "{cm[0][0]:,}\n",
    "({cm[0][0]/total*100:.1f}%)\",\n",
    "             f\"FP\n",
    "{cm[0][1]:,}\n",
    "({cm[0][1]/total*100:.1f}%)\"],\n",
    "            [f\"FN\n",
    "{cm[1][0]:,}\n",
    "({cm[1][0]/total*100:.1f}%)\",\n",
    "             f\"TP\n",
    "{cm[1][1]:,}\n",
    "({cm[1][1]/total*100:.1f}%)\"]\n",
    "        ])\n",
    "        color_vals = np.array([[0.75, 0.25], [0.25, 0.75]])\n",
    "        sns.heatmap(color_vals, annot=annot, fmt=\"\", ax=ax, cmap=\"RdYlGn\",\n",
    "                    vmin=0, vmax=1, cbar=False,\n",
    "                    xticklabels=[\"Pred: Allow\", \"Pred: Block\"],\n",
    "                    yticklabels=[\"True: Benign\", \"True: Attack\"],\n",
    "                    annot_kws={\"fontsize\": 10, \"fontweight\": \"bold\"})\n",
    "        ax.set_title(model, fontsize=13, fontweight=\"bold\", pad=10)\n",
    "\n",
    "    for j in range(len(models), len(axes)): axes[j].set_visible(False)\n",
    "    fig.suptitle(SCENARIO_TITLES[s_key], fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(FIGURES_DIR, f\"fig_cm_{s_key}.png\"), bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
